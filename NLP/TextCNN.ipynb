{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.num_class = config.num_clas\n",
    " \n",
    "        self.embedding = nn.Embedding(num_embeddings=config.vocab_size, \n",
    "                                embedding_dim=config.embedding_size)  \n",
    "        self.convs = nn.ModuleList([\n",
    "                       nn.Sequential(\n",
    "                \t\t\t  nn.Conv1d(in_channels=config.embedding_size,\n",
    "                \t\t\t  out_channels=config.out_channels, \n",
    "                              kernel_size= ks),  \t\t\t\t\t\t\n",
    "                              nn.ReLU(), \n",
    "                              nn.MaxPool1d(kernel_size=config.max_len-ks+1)) \n",
    "                       for ks in config.kernel_size ]) # 创建3个nn.Sequential，包含了 图中的convolution层、activation function层 和 maxPooling层, 其中每个层的参数都有变化\n",
    "        self.fc = nn.Linear(in_features=config.out_channels*len(config.kernel_size),\n",
    "                            out_features=config.num_class) # 每种类别的卷积核个数相乘，得到的长度就是全连接层输入的长度 \n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_x = self.embedding(x) # b x src_len\n",
    "        embed_x = embed_x.permute(0, 2, 1) \n",
    "        # b x src_len x embed_size --> b x embed_size x src_lem\n",
    "        out = [conv(embed_x) for conv in self.convs]  #计算每层卷积的结果，这里输出的结果已经经过池化层处理了\n",
    "        out = torch.cat(out, dim=1)  # 对池化后的向量进行拼接\n",
    "        out = out.view(-1, out.size(1)) # 拉成一竖条作为全连接层的输入\n",
    "        out = F.dropout(input=out, p=self.dropout_rate) # 这里也没有在图中的表现出来，这里是随机让一部分的神经元失活，避免过拟合。它只会在train的状态下才会生效。进入train状态可查看nn.Module。train()方法\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "425f96c65d6abe0af8f5a574b544c136a84d384aab4da1ae2e2902dd1c17bf8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
